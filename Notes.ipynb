{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ebf5cf",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Finance\" data-toc-modified-id=\"Finance-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Finance</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Modern-Portfolio-Theory\" data-toc-modified-id=\"Modern-Portfolio-Theory-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Modern Portfolio Theory</a></span></li><li><span><a href=\"#Linear-&amp;-Quadratic-Programming\" data-toc-modified-id=\"Linear-&amp;-Quadratic-Programming-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Linear &amp; Quadratic Programming</a></span></li><li><span><a href=\"#Technical-Analysis\" data-toc-modified-id=\"Technical-Analysis-1.0.3\"><span class=\"toc-item-num\">1.0.3&nbsp;&nbsp;</span>Technical Analysis</a></span></li></ul></li></ul></li><li><span><a href=\"#Machine-Learning\" data-toc-modified-id=\"Machine-Learning-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Machine Learning</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Principal-component-analysis-(PCA)\" data-toc-modified-id=\"Principal-component-analysis-(PCA)-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>Principal component analysis (PCA)</a></span></li><li><span><a href=\"#Monte-Carlo-Simulation\" data-toc-modified-id=\"Monte-Carlo-Simulation-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>Monte Carlo Simulation</a></span></li><li><span><a href=\"#Clustering-with-KMeans\" data-toc-modified-id=\"Clustering-with-KMeans-2.0.3\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>Clustering with KMeans</a></span><ul class=\"toc-item\"><li><span><a href=\"#KMeans-on-Haidt's-Dataset\" data-toc-modified-id=\"KMeans-on-Haidt's-Dataset-2.0.3.1\"><span class=\"toc-item-num\">2.0.3.1&nbsp;&nbsp;</span>KMeans on Haidt's Dataset</a></span></li><li><span><a href=\"#Haidt-(day-2)\" data-toc-modified-id=\"Haidt-(day-2)-2.0.3.2\"><span class=\"toc-item-num\">2.0.3.2&nbsp;&nbsp;</span>Haidt (day 2)</a></span></li><li><span><a href=\"#Haidt-(day-3)\" data-toc-modified-id=\"Haidt-(day-3)-2.0.3.3\"><span class=\"toc-item-num\">2.0.3.3&nbsp;&nbsp;</span>Haidt (day 3)</a></span></li></ul></li><li><span><a href=\"#NLP---Carbuying-Forum\" data-toc-modified-id=\"NLP---Carbuying-Forum-2.0.4\"><span class=\"toc-item-num\">2.0.4&nbsp;&nbsp;</span>NLP - Carbuying Forum</a></span></li><li><span><a href=\"#Regression\" data-toc-modified-id=\"Regression-2.0.5\"><span class=\"toc-item-num\">2.0.5&nbsp;&nbsp;</span>Regression</a></span></li><li><span><a href=\"#simple-Neural-Net\" data-toc-modified-id=\"simple-Neural-Net-2.0.6\"><span class=\"toc-item-num\">2.0.6&nbsp;&nbsp;</span>simple Neural Net</a></span></li><li><span><a href=\"#K-Nearest-Neighbors\" data-toc-modified-id=\"K-Nearest-Neighbors-2.0.7\"><span class=\"toc-item-num\">2.0.7&nbsp;&nbsp;</span>K-Nearest Neighbors</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63a1504",
   "metadata": {},
   "source": [
    "Abstract: These are my personal Python notes which i) illustrate my competencies and programming style and ii) serve as a useful resource for me as I continue to practice and improve at software development. It's compiled from the independent programming projects I've tinkered with since finishing at University of Waterloo in April '22; projects I've worked on to bridge the gap between my relatively strong mathematics background and my (admittedly weaker) background in software development.\n",
    "\n",
    "It's possible these notes may be unpolished in a few sections. I'm writing this on 2022-07-07 and I plan to publish them very soon; as some jobs I'd like to apply to have deadlines coming up, and I'm hoping this project can demonstrate to employers that I can develop readable and scalable software. By the middle of July everything in this .ipynb will be clear, well organized, and will follow best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e368a",
   "metadata": {},
   "source": [
    "# Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a8aa01",
   "metadata": {},
   "source": [
    "### Modern Portfolio Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62543470",
   "metadata": {},
   "source": [
    "Consider a portfolio with mean asset return $\\mu_i$ for $i \\in (1, ..., n)$ and covariance between returns of $i$ and $j$ denoted by $\\Sigma$, which we assume to be symmetric and positive semidefinite (and positive definite when specified).\n",
    "\n",
    "Portfolio return and variance is\n",
    "$$\\mu_{p}=\\mu^{\\prime} x \\text { and } \\sigma_{p}^{2}=x^{\\prime} \\Sigma x$$\n",
    "for a given $x \\in \\mathbb{R}^n$ representing weights of each asset in the portfolio.\n",
    "\n",
    "The goal is to select $x$ subject to the budget constraint $\\mathbb{1}^{\\prime} x = 1$ and non-negativity constraint $x \\geq \\mathbb{0}$ that gives large $\\mu_{p}$ and small $\\sigma_p^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a849410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader as pdr\n",
    "\n",
    "tickers = ['msft', 'tqqq', 'tsla', 'spy', 'aapl', 'f', 'twtr', 'ibm']\n",
    "NoA = len(tickers)\n",
    "\n",
    "df = pdr.DataReader(tickers, data_source='yahoo', start='2017-01-01', end='2020-09-28')['Adj Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e78884",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfret = df.pct_change().dropna()\n",
    "\n",
    "display((dfret).head(3))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "arr = dfret.to_numpy()\n",
    "\n",
    "#print(np.cov(arr.T))\n",
    "\n",
    "corr_df = dfret.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9638c512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "seaborn.heatmap(corr_df, annot=True, cmap='RdYlGn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b713404",
   "metadata": {},
   "source": [
    "$\\Sigma = [\\sigma_{i j}] = [\\sigma_i \\sigma_j \\rho_{i j}]$\n",
    "\n",
    "$\\rho = \\left[\\begin{array}{cccccccc}\n",
    "1 & 0.92 & 0.43 & 0.85 & 0.75 & 0.38 & 0.48 & 0.61 \\\\\n",
    "0.92 & 1 & 0.5 & 0.93 & 0.86 & 0.47 & 0.53 & 0.67 \\\\\n",
    "0.43 & 0.5 & 1 & 0.43 & 0.41 & 0.29 & 0.28 & 0.27 \\\\\n",
    "0.85 & 0.93 & 0.43 & 1 & 0.78 & 0.62 & 0.5 & 0.76 \\\\\n",
    "0.75 & 0.86 & 0.41 & 0.78 & 1 & 0.37 & 0.43 & 0.53 \\\\\n",
    "0.38 & 0.47 & 0.29 & 0.62 & 0.37 & 1 & 0.34 & 0.51 \\\\\n",
    "0.48 & 0.53 & 0.28 & 0.5 & 0.43 & 0.34 & 1 & 0.34 \\\\\n",
    "0.61 & 0.67 & 0.27 & 0.76 & 0.53 & 0.51 & 0.34 & 1\n",
    "\\end{array}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8586608f",
   "metadata": {},
   "source": [
    "We will simulate $10,000$ random portfolios by defining a random $10,000 \\times 8$ array (namely rand2d) such that each row sums to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52739c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "rand2d = np.random.random((10000,NoA))\n",
    "rand2d = rand2d/rand2d.sum(axis=1,keepdims=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98b385e",
   "metadata": {},
   "source": [
    "We can calculate portfolio returns and standard deviations with these formulae:\n",
    "\n",
    "$\\mathrm{E}\\left(R_{p}\\right)=\\sum_{i} w_{i} \\mathrm{E}\\left(R_{i}\\right)$\n",
    "\n",
    "$\\sigma_{p}^{2}=\\sum_{j, i} w_{i} w_{j} \\sigma_{i} \\sigma_{j} \\rho_{i j}=\\sum_{i} w_{i}^{2} \\sigma_{i}^{2}+\\sum_{i} \\sum_{j \\neq i} w_{i} w_{j} \\sigma_{i} \\sigma_{j} \\rho_{i j}$, since $\\rho_{i i} = 1$\n",
    "\n",
    "Additionally we need to multiply by the number of trading days in a year to define yearly paramaters rather than daily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6625d827",
   "metadata": {},
   "outputs": [],
   "source": [
    "retSimulated = rand2d.dot(dfret.mean())*252\n",
    "volSimulated = np.sqrt(np.diag(rand2d@(dfret.cov() * 252@rand2d.T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df5ef0",
   "metadata": {},
   "source": [
    "We define the risk free rate to be $0$, and the Sharpe ratio calculation is thus $\\frac{R_p}{\\sigma_p}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068a406",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpeSimulated = retSimulated / volSimulated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0962b2a1",
   "metadata": {},
   "source": [
    "Now we can plot the graph - along with the highest sharpe ratio portfolio and lowest variance portfolio denoted with stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1072d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sharpeSimulated = retSimulated / volSimulated\n",
    "\n",
    "plt.figure(figsize=(11, 5))\n",
    "plt.scatter(volSimulated, retSimulated, c=sharpeSimulated, marker='.')\n",
    "plt.grid(True)\n",
    "\n",
    "max_sr_vol = volSimulated[np.argmax(sharpeSimulated)]\n",
    "max_sr_ret = retSimulated[np.argmax(sharpeSimulated)]\n",
    "\n",
    "min_ri_vol = volSimulated[np.argmin(volSimulated)]\n",
    "min_ri_ret = retSimulated[np.argmin(volSimulated)]\n",
    "\n",
    "\n",
    "plt.scatter(max_sr_vol, max_sr_ret, c=\"orange\", marker='*',\n",
    "            s=160, label=\"Max Sharpe Ratio Portfolio\")\n",
    "\n",
    "plt.scatter(min_ri_vol, min_ri_ret, c=\"purple\",\n",
    "            s=160, marker='*', label=\"Min Risk Portfolio\")\n",
    "\n",
    "\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.2, 1))\n",
    "\n",
    "plt.xlabel('expected volatility')\n",
    "plt.ylabel('expected return')\n",
    "plt.colorbar(label='Sharpe ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d34f3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb146a8d",
   "metadata": {},
   "source": [
    "The optimization equation that we will use to model the frontier in the next cell is taken from here: https://github.com/cantaro86/Financial-Models-Numerical-Methods/blob/master/7.1%20Classical%20MVO.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad16a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import Bounds, LinearConstraint, minimize\n",
    "\n",
    "def optimizer(MU, COV, target_mu, OnlyLong=True):\n",
    "    \"\"\" Finds optimal weights for a fixed target portfolio return \"\"\"\n",
    "    \n",
    "    N = len(MU)\n",
    "    if OnlyLong == True:\n",
    "        bounds = Bounds(0, 1)\n",
    "    A = np.vstack( (np.ones(N), MU) )\n",
    "    B = np.array([1,target_mu])\n",
    "    linear_constraint = LinearConstraint( A, B, B)\n",
    "    \n",
    "    weights = np.ones(N)\n",
    "    x0 = weights/np.sum(weights) #Create x0, the initial guess for the weights\n",
    "\n",
    "    #Define the objective function\n",
    "    quadratic_form = lambda w: (w.T @ COV @ w) \n",
    "    if OnlyLong:\n",
    "        res = minimize(quadratic_form, x0=x0, method='trust-constr', constraints=linear_constraint, bounds=bounds)\n",
    "    else:\n",
    "        res = minimize(quadratic_form, x0=x0, method='trust-constr', constraints=linear_constraint)\n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabebca7",
   "metadata": {},
   "source": [
    "We again plot our random portfolios but with the adjustments\n",
    "- Risk free rate is $3$% annually\n",
    "- Add the security market line\n",
    "- Add the efficient frontier, computed with optimizer defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc7dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MU = dfret.mean()*252\n",
    "COV = dfret.cov()*252\n",
    "Rf = 0.03\n",
    "\n",
    "samples = 200\n",
    "means = np.linspace(0, np.max(MU), samples)       # vector of target expected returns\n",
    "stds = np.zeros_like(means)\n",
    "sharpe_ratio = np.zeros_like(means)\n",
    "\n",
    "for i,mn in enumerate(means):\n",
    "    w_opt = optimizer(MU, COV, mn)                 # optimal weights\n",
    "    stds[i] = np.sqrt(w_opt@COV@w_opt)\n",
    "    sharpe_ratio[i] = (mn - Rf)/stds[i]\n",
    "    \n",
    "ind_SR = np.argmax(sharpe_ratio)      # index of the maximum Sharpe Ratio\n",
    "max_SR = sharpe_ratio[ind_SR]         # maximum Sharpe ratio\n",
    "\n",
    "y = np.linspace(0, stds.max(), samples)\n",
    "CML = Rf + max_SR * y       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257a7cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand2d = np.random.random((10000,NoA))\n",
    "rand2d = rand2d/rand2d.sum(axis=1,keepdims=1)\n",
    "\n",
    "retSimulated = rand2d.dot(dfret.mean())*252\n",
    "volSimulated = np.sqrt(np.diag(rand2d@(dfret.cov() * 252@rand2d.T)))\n",
    "sharpeSimulated = retSimulated / volSimulated\n",
    "\n",
    "plt.figure(figsize=(11, 5))\n",
    "plt.scatter(volSimulated, retSimulated, c=sharpeSimulated, marker='.') # plots random portfolios\n",
    "\n",
    "plt.scatter(stds, means, c='green', marker='.', label='Efficient frontier, computed by optimizer')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlabel('expected volatility') # we can incorporate latex with r'$E(\\sigma_P)$'\n",
    "plt.ylabel('expected return')\n",
    "plt.colorbar(label='Sharpe ratio')\n",
    "plt.plot(y, CML, label='Security Market Line')\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.2, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945dd25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b7cf60",
   "metadata": {},
   "source": [
    "Now we can display the optimal portolio weightings according to modern portfolio theory.\n",
    "\n",
    "The results are reasonable when we consider how well msft, tsla, and aapl over our timeperiod - as well as how poorly the other stocks did relatively.\n",
    "\n",
    "This portfolio's expected return and volatility lie on the SML and the efficient frontier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff15931",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "display(pd.DataFrame( [dict(zip( tickers, optimizer(MU, COV, means[ind_SR]).round(4)))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2043ba",
   "metadata": {},
   "source": [
    "We incorporate the following equations from http://www.math.utah.edu/~zhu/5765.17s/week10.pdf which solve for the optimal portfolio when short selling is permitted, ie we drop the constraint that $\\mathbf{x} \\geq \\mathbb{0}$\n",
    "\n",
    "$\\begin{aligned}\n",
    "&A=\\mathbf{l}^{T} \\Sigma^{-1} \\mathbf{l}>0 \\\\\n",
    "&B=\\boldsymbol{\\mu}^{T} \\Sigma^{-1} \\mathbf{l} \\\\\n",
    "&C=\\mu^{T} \\Sigma^{-1} \\boldsymbol{\\mu}>0\n",
    "\\end{aligned}$\n",
    "\n",
    "$\\mathbf{x}_{\\mu}=\\left(\\frac{C-\\mu B}{A C-B^{2}}\\right) \\Sigma^{-1} \\mathbf{l}+\\left(\\frac{\\mu A-B}{A C-B^{2}}\\right) \\Sigma^{-1} \\boldsymbol{\\mu}$, where $A C - B^2 > 0$\n",
    "\n",
    "$\\sigma_{P}^{2}(\\mu)= \\mathbf{x}_{\\mu}^T \\Sigma \\mathbf{x}_{\\mu}$\n",
    "\n",
    "$\\sigma_{P}^{2}(\\mu)=\\frac{A \\mu^{2}-2 B \\mu+C}{A C-B^{2}}$\n",
    "\n",
    "$\\sigma_{P} = \\sqrt{\\frac{A \\mu^{2}-2 B \\mu+C}{A C-B^{2}}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1796812",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.ones_like(MU)\n",
    "A = l.T@np.linalg.inv(COV)@l\n",
    "B = MU.T@np.linalg.inv(COV)@l\n",
    "C = MU.T@np.linalg.inv(COV)@MU\n",
    "\n",
    "def vol_P(mu_target):\n",
    "    return (A*mu_target**2-2*B*mu_target+C)/(A*C-B*B)\n",
    "\n",
    "print(A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33aadd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand2d = np.random.random((10000,NoA))\n",
    "rand2d = rand2d/rand2d.sum(axis=1,keepdims=1)\n",
    "\n",
    "retSimulated = rand2d.dot(dfret.mean())*252\n",
    "volSimulated = np.sqrt(np.diag(rand2d@(dfret.cov() * 252@rand2d.T)))\n",
    "sharpeSimulated = retSimulated / volSimulated\n",
    "\n",
    "plt.figure(figsize=(11, 5))\n",
    "plt.scatter(volSimulated, retSimulated, c=sharpeSimulated, marker='.')\n",
    "\n",
    "plt.scatter(stds, means, c='green', marker='.', label=\"w/o short selling permitted\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlabel('expected volatility')\n",
    "plt.ylabel('expected return')\n",
    "plt.colorbar(label='Sharpe ratio')\n",
    "x = np.linspace(0, means.max(), samples)\n",
    "plt.plot(vol_P(x)**.5, x, label=\"efficient frontier w/ short selling permitted.\")\n",
    "\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1.2, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6f990a",
   "metadata": {},
   "source": [
    "### Linear & Quadratic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d648c0",
   "metadata": {},
   "source": [
    "$$\\max \\left(\\begin{array}{llllll}2 & 1 & 2 & 5 & 7 & 8\\end{array}\\right) x$$\n",
    "$$\\text{s.t.} $$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\left(\\begin{array}{llllll}\n",
    "2 & 1 & 2 & 3 & 5 & 4 \\\\\n",
    "0 & 1 & 2 & 2 & 1 & 4 \\\\\n",
    "3 & 1 & 3 & 2 & 2 & 4\n",
    "\\end{array}\\right) x \\leq\\left(\\begin{array}{c}\n",
    "170 \\\\\n",
    "160 \\\\\n",
    "95\n",
    "\\end{array}\\right) \\\\\n",
    "&x \\geq \\mathbb{0} .\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4032df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.optimize import Bounds, LinearConstraint, minimize\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([\n",
    "    [2, 1, 2, 3, 5, 4],\n",
    "    [0, 1, 2, 2, 1, 4],\n",
    "    [3, 1, 3, 2, 2, 4]])\n",
    "\n",
    "b = np.array([170, 160, 95])\n",
    "\n",
    "c = [2, 1, 2, 5, 7, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d4c4d4",
   "metadata": {},
   "source": [
    "Since scipy.optimize.linprog solves for $\\operatorname{min}c^{T}x$ and we are solving for $\\operatorname{max}c^{T}x$, we can equivalently solve for $\\operatorname{min}{-c}^{T}x \\text{  s.t.  } \\{\\ldots\\}$ to get our desired $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f9aee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neg_c = np.zeros_like(c) - c\n",
    "res = scipy.optimize.linprog(neg_c, A_ub=A, b_ub=b, bounds=(0, None))\n",
    "\n",
    "x_optimal = res.x\n",
    "print(\"Our optimal x is: \" + str(x_optimal.round(4)) + \".T\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b5f1fe",
   "metadata": {},
   "source": [
    "We can test a few values to verify optimality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc67d2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "items = [0, 1]\n",
    "\n",
    "ep = 0.001\n",
    "\n",
    "print(c@x_optimal)\n",
    "\n",
    "def optimal_scaled(arr, b, c):\n",
    "    if np.all(A@arr-b < 0 for i in A@arr-b):\n",
    "        return arr\n",
    "    else:\n",
    "        return optimal_scaled(arr*.99999, b, c)\n",
    "\n",
    "for item in product(items, repeat=6):\n",
    "    tmp = optimal_scaled((x_optimal+ep*(2*np.array(item)-1)), b, c)\n",
    "    if c@tmp > c@x_optimal:\n",
    "        print(A@tmp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a03068",
   "metadata": {},
   "source": [
    "$$\\max \\quad x^T \n",
    "\\left(\\begin{array}{llllll}\n",
    "1 & 1 & 3 & 6 & 2 & 4 \\\\\n",
    "4 & 4 & 4 & 6 & 5 & 2 \\\\\n",
    "1 & 2 & 1 & 5 & 3 & 2 \\\\\n",
    "5 & 2 & 6 & 0 & 1 & 1 \\\\\n",
    "3 & 4 & 5 & 4 & 4 & 5 \\\\\n",
    "1 & 3 & 3 & 5 & 6 & 0\n",
    "\\end{array}\\right)x \\quad\n",
    "$$\n",
    "$$\\text{s.t.} $$\n",
    "$$\n",
    "\\\\\n",
    "\\begin{aligned}\n",
    "&\\left(\\begin{array}{llllll}\n",
    "2 & 1 & 2 & 3 & 5 & 4 \\\\\n",
    "0 & 1 & 2 & 2 & 1 & 4 \\\\\n",
    "3 & 1 & 3 & 2 & 2 & 4\n",
    "\\end{array}\\right) x \\leq\\left(\\begin{array}{c}\n",
    "170 \\\\\n",
    "160 \\\\\n",
    "95\n",
    "\\end{array}\\right) \\\\\n",
    "\\text{ and } \\quad &x = \\mathbb{0} .\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad71da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.array([[1, 1, 3, 0, 2, 4],\n",
    "       [4, 4, 4, 6, 5, 2],\n",
    "       [1, 2, 1, 5, 3, 2],\n",
    "       [5, 2, 6, 0, 1, 1],\n",
    "       [3, 4, 5, 4, 4, 5],\n",
    "       [1, 3, 3, 5, 6, 0]])\n",
    "\n",
    "A = np.array([\n",
    "    [2, 1, 2, 3, 5, 4],\n",
    "    [0, 1, 2, 2, 1, 4],\n",
    "    [3, 1, 3, 2, 2, 4]])\n",
    "\n",
    "b = np.array([170, 160, 95])\n",
    "\n",
    "fn = lambda x: (x.T@Q@x) \n",
    "\n",
    "res = scipy.optimize.minimize(fn, x0=[0, 0, 0, 0, 0, 0], method='trust-constr',\n",
    "                        bounds=Bounds(-2,2), constraints=LinearConstraint(A, b, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6864d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.fun)\n",
    "x = res.x\n",
    "print(x)\n",
    "print(x.T@Q@x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e9b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import Bounds, LinearConstraint, minimize\n",
    "\n",
    "def optimizer(MU, COV, target_mu, OnlyLong=True):\n",
    "    \"\"\" Finds optimal weights for a fixed target portfolio return \"\"\"\n",
    "    \n",
    "    N = len(MU)\n",
    "    if OnlyLong == True:\n",
    "        bounds = Bounds(0, 1)\n",
    "    A = np.vstack( (np.ones(N), MU) )\n",
    "    B = np.array([1,target_mu])\n",
    "    linear_constraint = LinearConstraint( A, B, B)\n",
    "    \n",
    "    weights = np.ones(N)\n",
    "    x0 = weights/np.sum(weights) #Create x0, the initial guess for the weights\n",
    "\n",
    "    #Define the objective function\n",
    "    quadratic_form = lambda w: (w.T @ COV @ w) \n",
    "    if OnlyLong:\n",
    "        res = minimize(quadratic_form, x0=x0, method='trust-constr', constraints=linear_constraint, bounds=bounds)\n",
    "    else:\n",
    "        res = minimize(quadratic_form, x0=x0, method='trust-constr', constraints=linear_constraint)\n",
    "    return res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd215ca",
   "metadata": {},
   "source": [
    "### Technical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47dac99",
   "metadata": {},
   "source": [
    "https://www.quantconnect.com/docs/algorithm-reference/indicators\n",
    "\n",
    "https://www.axi.com/int/blog/education/technical-indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bea3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader as pdr\n",
    "\n",
    "tickers = ['msft', 'tqqq']\n",
    "\n",
    "df = pdr.DataReader(tickers, data_source='yahoo',\n",
    "                    start='2017-01-01', end='2020-09-28')['Adj Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b719f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.arange(941)\n",
    "y = df[\"msft\"]\n",
    "\n",
    "arr = np.zeros((2, 941))\n",
    "\n",
    "arr[0] = x\n",
    "arr[1] = y\n",
    "\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ec653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of squared difference S_xy\n",
    "def S(x, y):\n",
    "    return np.sum((x-np.mean(x))*(y-np.mean(y)))\n",
    "\n",
    "beta_1 = S(x,y)/S(x,x)\n",
    "beta_0 = np.mean(y) - beta_1*np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b7138f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "x_line = np.linspace(0,940,941)\n",
    "y_line = beta_1*x+beta_0\n",
    "plt.plot(x_line, y_line, '-r')\n",
    "plt.plot(arr[0], arr[1])\n",
    "#plt.plot(arr[0][0::10], arr[1][0::10])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade2df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Moving Average Indicator (MA)\n",
    "\n",
    "n = 80\n",
    "\n",
    "ma_method1 = np.convolve(arr[1], np.ones(n)/n, mode='valid')\n",
    "ma_method2 = y.rolling(window=n).mean()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(np.arange(len(y)), y)\n",
    "plt.plot(np.arange(len(ma_method1))+n, ma_method1, '-r')\n",
    "plt.plot(np.arange(len(ma_method2)), ma_method2, '-r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2465f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Exponential Moving Average Indicator (EMA)\n",
    "\n",
    "n = 26\n",
    "\n",
    "times = arr[1]\n",
    "ewma = df['msft'].ewm(span=n, adjust=False, min_periods=n).mean()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(np.arange(len(y)), y)\n",
    "plt.plot(np.arange(len(ewma)), ewma, '-r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284489ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Moving Average Convergence Divergence (MACD)\n",
    "\n",
    "ewma_12 = df['msft'].ewm(span=12, adjust=False, min_periods=12).mean()\n",
    "ewma_26 = df['msft'].ewm(span=26, adjust=False, min_periods=26).mean()\n",
    "\n",
    "macd = ewma_12 - ewma_26\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(np.arange(len(macd)), macd)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03a34a3",
   "metadata": {},
   "source": [
    "$R S I=100-100 /(1+R S)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c8f550",
   "metadata": {},
   "source": [
    "$R S I_{\\text {step one }}=100-\\left[\\frac{100}{1+\\frac{\\text { Average gain }}{\\text { Average loss }}}\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0445dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pdr.DataReader('msft', data_source='yahoo',\n",
    "                    start='2017-01-01', end='2020-09-28') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50bc84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Relative Strength Index (RSI)\n",
    "\n",
    "import pandas_ta as ta\n",
    "\n",
    "n = 14\n",
    "x = arr[1]\n",
    "\n",
    "def avg_gain(arr):\n",
    "    return np.mean(arr[arr >= 0])\n",
    "    \n",
    "def avg_loss(arr):\n",
    "    return np.mean(arr[arr < 0])\n",
    "\n",
    "RSI = np.zeros(len(x)+100)\n",
    "\n",
    "for i in range(len(RSI)):\n",
    "    RSI[i] = 100-100/(1+abs(avg_gain(np.diff(x[i:i+n]) / x[i:i+n][:-1])/\n",
    "                      avg_loss(np.diff(x[i:i+n]) / x[i:i+n][:-1])))\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(RSI, '-b')\n",
    "plt.show()\n",
    "\n",
    "# __________________________ \n",
    "rsi = np.array(df[[\"Close\"]].ta.rsi())\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(np.arange(len(rsi)), rsi, '-b')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a312bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Percentage Price Oscillator indicator (PPO)\n",
    "\n",
    "ewma_12 = df['Adj Close'].ewm(span=12, adjust=False, min_periods=12).mean()\n",
    "ewma_26 = df['Adj Close'].ewm(span=26, adjust=False, min_periods=26).mean()\n",
    "\n",
    "macd = ewma_12 - ewma_26\n",
    "\n",
    "PPO = macd/ewma_26*100\n",
    "\n",
    "PPO_signal = PPO.ewm(span=9, adjust=False, min_periods=9).mean()\n",
    "PPO_hist = PPO - PPO_signal\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(np.arange(len(PPO)), PPO)\n",
    "plt.plot(np.arange(len(macd)), macd)\n",
    "plt.plot(np.arange(len(PPO_signal)), PPO_signal)\n",
    "plt.plot(np.arange(len(PPO_hist)), PPO_hist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade721bb",
   "metadata": {},
   "source": [
    "$S A R_{n+1}=S A R_{n}+\\alpha\\left(E P-S A R_{n}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d7d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Parabolic SAR indicator (PSAR)\n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "df = pdr.DataReader('msft', data_source='yahoo',\n",
    "                    start='2017-01-01', end='2020-09-28') \n",
    "\n",
    "psar = ta.psar(high=df['High'], low=df['Low'], close=df['Close'], af0=0.02, af=0.02, max_af=0.2)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(np.arange(len(psar)), psar['PSARl_0.02_0.2'], '-g')\n",
    "plt.plot(np.arange(len(psar)), psar['PSARs_0.02_0.2'], '-r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca012c2",
   "metadata": {},
   "source": [
    "https://github.com/QuantConnect/Lean/blob/master/Indicators/AverageDirectionalIndex.cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4a9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Average Directional Index (ADX)\n",
    "\n",
    "adx = df.ta.adx(high=df['High'], low=df['Low'], close=df['Close'])\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(np.arange(len(adx)), adx['ADX_14'], '-g')\n",
    "#plt.plot(np.arange(len(adx)), adx['DMP_14'], '-r')\n",
    "#plt.plot(np.arange(len(adx)), adx['DMN_14'], '-b')\n",
    "#plt.axhline(20,color='red')\n",
    "#plt.axhline(25,color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ed0d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 8. Stochastic Oscillator Indicator\n",
    "\n",
    "stoch = df.ta.stoch(high=df['High'], low=df['Low'], close=df['Close'])\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(np.arange(len(stoch)), stoch['STOCHk_14_3_3'], '-g')\n",
    "#plt.plot(np.arange(len(stoch)), stoch['STOCHd_14_3_3'], '-r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786bb4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Bollinger Bands Indicators\n",
    "\n",
    "#bbands = ta.bbands(high=df['High'], low=df['Low'], close=df['Close'])\n",
    "bbands = ta.bbands(high=df[0:100]['High'], low=df[0:100]['Low'], close=df[0:100]['Close'])\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(np.arange(len(bbands)), bbands['BBL_5_2.0'], '-g')\n",
    "plt.plot(np.arange(len(bbands)), bbands['BBM_5_2.0'], '-r')\n",
    "plt.plot(np.arange(len(bbands)), bbands['BBU_5_2.0'], '-b')\n",
    "#plt.plot(np.arange(len(bbands)), bbands['BBB_5_2.0'], '-')\n",
    "#plt.plot(np.arange(len(bbands)), bbands['BBP_5_2.0'], '-y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e89fc14",
   "metadata": {},
   "source": [
    "Calculation\n",
    "1. Calculate the SMA for Period $n$\n",
    "2. Subtract the SMA value from step one from the Close for each of the past $n$ Periods and square them\n",
    "3. Sum the squares of the differences and divide by $n$\n",
    "4. Calculate the square root of the result from step three\n",
    "$\\mathrm{SD}=$ Sqrt $\\left[\\left(\\right.\\right.$ Sum the $\\left.\\left.\\left((\\text { Close for each of the past } n \\text { Periods }-n \\text { Period SMA for current bar })^{\\wedge} 2\\right)\\right) / n\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e7a2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 10. Standard Deviation Indicator\n",
    "\n",
    "import scipy\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "n = 80\n",
    "\n",
    "SMA = df['Adj Close'].rolling(window=n).mean()\n",
    "sdArr = df['Adj Close'].rolling(window=n).std()\n",
    "\n",
    "diff = np.array(SMA) - np.array(df['Adj Close'])\n",
    "diff_squared = np.power(diff, 2)\n",
    "\n",
    "diff_squared_rolling = np.convolve(diff_squared, np.ones(n)/n, mode='valid')\n",
    "sqrt_dsr = np.power(diff_squared_rolling, 0.5)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(np.arange(len(df['Adj Close'])), df['Adj Close'], '-b')\n",
    "plt.plot(np.arange(len(df['Adj Close'])), SMA, '-r')\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(np.arange(len(sqrt_dsr)), sqrt_dsr, '-g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ed1899",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89cba2",
   "metadata": {},
   "source": [
    "### Principal component analysis (PCA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c7deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Misc/StudentsPerformance.csv')\n",
    "df = df[['math score','writing score', 'reading score']]\n",
    "\n",
    "df = pd.DataFrame(df).to_numpy()\n",
    "\n",
    "x1 = np.zeros(len(df))\n",
    "x2 = np.zeros(len(df))\n",
    "x3 = np.zeros(len(df))\n",
    "\n",
    "for i in range(len(df)):\n",
    "    x1[i] = df[i][0]\n",
    "    x2[i] = df[i][1]\n",
    "    x3[i] = df[i][2]\n",
    "\n",
    "x1 = x1 - np.mean(x1)\n",
    "x2 = x2 - np.mean(x2)\n",
    "x3 = x3 - np.mean(x3)\n",
    "\n",
    "df = pd.DataFrame({'Column1': x1, 'Column2': x2, 'Column3': x3})\n",
    "\n",
    "sm = np.array([[0, 0, 0],\n",
    "               [0, 0, 0],\n",
    "               [0, 0, 0]])\n",
    "for i in range(len(df)):\n",
    "    sm = sm + np.outer(np.array([x1[i] , x2[i], x3[i]]), np.array([x1[i] , x2[i], x3[i]]))\n",
    "sm = sm/len(df)\n",
    "\n",
    "U = np.linalg.svd(sm)[0]\n",
    "print(U.T)\n",
    "\n",
    "x_trend = np.linspace(min(x1), max(x1), 100)\n",
    "y_trend = U.T[0][1]/U.T[0][0]*x_trend\n",
    "\n",
    "plt.scatter(x1, x2)\n",
    "plt.plot(x_trend, y_trend, '-b')\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "fig = plt.figure(figsize=(4, 8))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "#plot the plane\n",
    "xx, yy = np.meshgrid(range(-60, 25), range(-60, 100))\n",
    "z = xx*U.T[0][2]/U.T[0][0]+yy*U.T[1][2]/U.T[1][0]\n",
    "ax.plot_surface(xx, yy, z, alpha=0.5)\n",
    "\n",
    "phi = np.linspace(min(x1), max(x1), 100)\n",
    "x_t = U.T[0][0]/U.T[0][0]*phi\n",
    "y_t = U.T[0][1]/U.T[0][0]*phi\n",
    "z_t = U.T[0][2]/U.T[0][0]*phi\n",
    "\n",
    "ax.plot(x_t, y_t, z_t, '-b')\n",
    "\n",
    "phi = np.linspace(min(x1), max(x1), 100)\n",
    "x_t = U.T[1][0]/U.T[1][0]*phi\n",
    "y_t = U.T[1][1]/U.T[1][0]*phi\n",
    "z_t = U.T[1][2]/U.T[1][0]*phi\n",
    "\n",
    "ax.plot(x_t, y_t, z_t, '-b')\n",
    "\n",
    "#plot the line\n",
    "'''phi = np.linspace(min(x1), max(x1), 100)\n",
    "x_t = U.T[0][0]/U.T[0][0]*phi\n",
    "y_t = U.T[0][1]/U.T[0][0]*phi\n",
    "z_t = U.T[0][2]/U.T[0][0]*phi\n",
    "\n",
    "ax.plot(x_t, y_t, z_t, '-b')\n",
    "'''\n",
    "\n",
    "ax.scatter(x1, x2, x3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5938a0eb",
   "metadata": {},
   "source": [
    "### Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a353ce6c",
   "metadata": {},
   "source": [
    "I wrote the price option function, which computes the Black-Scholes option value using a binomial tree, for an assignment for my [Computational Methods in Business and Finance](https://uwflow.com/course/cs335) class at University of Waterloo. It utilizes dynamic programming methods for the American option pricing, though at time of writing there may be some revisions needed for that function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2020744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def price_option(S0, K, T, r, sigma, opttype, Nsteps, american):\n",
    "    delt = T/Nsteps;\n",
    "    # tree parameters\n",
    "    u = np.exp(sigma * np.sqrt(delt) );\n",
    "    d = 1./u;\n",
    "    a = np.exp( r*delt );\n",
    "    p = (a - d)/(u - d);\n",
    "    # payoff at t=T\n",
    "    W = S0 * d**(np.arange(Nsteps,-1,-1)) * u**(np.arange(Nsteps+1))\n",
    "    # W is column vector of length Nsteps+1\n",
    "    if opttype == 0:\n",
    "        W = np.maximum( W - K, 0);\n",
    "    else:\n",
    "        W = np.maximum( K - W, 0);\n",
    "    # backward recursion\n",
    "    for i in np.arange(Nsteps,0,-1):\n",
    "        W = np.exp(-r*delt)*( p*W[1:i+1] + (1-p)*W[0:i] )\n",
    "        if american == 1:\n",
    "            if opttype == 0:\n",
    "                S = np.maximum(-K+S0 * d**(np.arange(i,-1,-1)) * u**(np.arange(i+1)), 0)\n",
    "            else:\n",
    "                S = np.maximum( K-S0 * d**(np.arange(i-1,-1,-1)) * u**(np.arange(i)), 0)\n",
    "            W = np.maximum(W, S)\n",
    "    return str(W[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd1678cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6282042479158183'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S0 = 90\n",
    "K = 100\n",
    "T = 1\n",
    "r = 0.06\n",
    "sigma = .12\n",
    "opttype = 0; # opttype - 0 for a call, otherwise a put\n",
    "american = 0; # american - 0 for European, otherwise American\n",
    "Nsteps = 10000\n",
    "\n",
    "price_option(S0, K, T, r, sigma, opttype, Nsteps, american)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb023f4",
   "metadata": {},
   "source": [
    "### Clustering with KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0520d0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from pylab import mpl, plt\n",
    "plt.style.use('seaborn')\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "np.random.seed(1000)\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=250, centers=4,\n",
    "random_state=500, cluster_std=1.25)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=4, random_state=0)\n",
    "model.fit(X) \n",
    "\n",
    "y_kmeans = model.predict(X)\n",
    "y_kmeans[:12] \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c39b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "model = GaussianMixture(n_components=4, random_state=0)\n",
    "model.fit(X)\n",
    "y_gm = model.predict(X)\n",
    "y_gm[:12]\n",
    "(y_gm == y_kmeans).all()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada9ca6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "n_samples = 100\n",
    "X, y = make_classification(n_samples=n_samples, n_features=2,\n",
    "n_informative=2, n_redundant=0,\n",
    "n_repeated=0, random_state=250)\n",
    "X[:5]\n",
    "\n",
    "X.shape\n",
    "(100, 2)\n",
    "y[:5]\n",
    "y.shape\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(X);\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x=X[:, 0], y=X[:, 1], c=y, cmap='coolwarm');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b96fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X, y)\n",
    "    \n",
    "model.predict_proba(X).round(4)[:5]\n",
    "pred = model.predict(X)\n",
    "pred\n",
    "pred == y\n",
    "accuracy_score(y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7901db19",
   "metadata": {},
   "source": [
    "#### KMeans on Haidt's Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f41bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('ALL_MFQ30.csv').dropna(axis='rows')\n",
    "df = df[df.Sex != '.']\n",
    "del df['Country']\n",
    "\n",
    "#  HARM_AVG  FAIRNESS_AVG  INGROUP_AVG  AUTHORITY_AVG  PURITY_AVG Sex\n",
    "dfx = df[['HARM_AVG', 'FAIRNESS_AVG']].to_numpy()\n",
    "dfy = df[['Sex']].to_numpy().astype(int)\n",
    "\n",
    "df = df.to_numpy()\n",
    "\n",
    "plt.scatter(dfx[:, 0], dfx[:, 1], c=dfy, cmap='coolwarm', s=10);\n",
    "plt.xlabel('Harm')\n",
    "plt.ylabel('Fairness')\n",
    "plt.title('Harm Fairness Sex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bff89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ALL_MFQ30.csv').dropna(axis='rows')\n",
    "df = df[df.Sex != '.']\n",
    "del df['Country']\n",
    "\n",
    "dfy = df[['Sex']].to_numpy().astype(int)\n",
    "\n",
    "df = df.to_numpy()\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=2, random_state=0)\n",
    "model.fit(df) \n",
    "\n",
    "y_kmeans = model.predict(df)\n",
    "\n",
    "#plt.figure(figsize=(6, 6))\n",
    "#plt.scatter(df[:, 0], df[:, 3], c=y_kmeans, cmap='coolwarm');\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "a = model.labels_\n",
    "b = dfy.flatten()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc5a47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('ALL_MFQ30.csv').dropna(axis='rows')\n",
    "df = df[df.Sex != '.']\n",
    "del df['Country']\n",
    "\n",
    "df['Kmeans'] = a\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"ticks\")\n",
    "sns.pairplot(df, hue=\"Kmeans\")\n",
    "\n",
    "df = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f725507",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%matplotlib notebook\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "# Plot a basic wireframe\n",
    "ax1.set_title('Colour by kmeans algorithm')\n",
    "ax1.scatter(df[:, 0], df[:, 1], df[:, 2], c=a, cmap='coolwarm')\n",
    "ax1.set_xlabel(\"Harm\")\n",
    "ax1.set_ylabel(\"Fairness\")\n",
    "ax1.set_zlabel(\"Ingroup\")\n",
    "\n",
    "\n",
    "ax2.scatter(df[:, 0], df[:, 1], df[:, 2], c=b, cmap='coolwarm')\n",
    "ax2.set_title('Colour by gender')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6923f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "ax3 = fig.add_subplot(121, projection='3d')\n",
    "ax4 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "ax3.set_title('Colour by kmeans algorithm')\n",
    "ax3.scatter(df[:, 2], df[:, 3], df[:, 4], c=a, cmap='coolwarm')\n",
    "ax3.set_xlabel(\"Ingroup\")\n",
    "ax3.set_ylabel(\"Authority\")\n",
    "ax3.set_zlabel(\"Purity\")\n",
    "\n",
    "\n",
    "ax4.scatter(df[:, 2], df[:, 3], df[:, 4], c=b, cmap='coolwarm')\n",
    "ax4.set_title('Colour by gender')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0921ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,6))\n",
    "\n",
    "ax3 = fig.add_subplot(121, projection='3d')\n",
    "ax4 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "ax3.set_title('Colour by kmeans algorithm')\n",
    "ax3.scatter(df[:, 2], df[:, 4], df[:, 0], c=a, cmap='coolwarm')\n",
    "ax3.set_xlabel(\"Ingroup\")\n",
    "ax3.set_ylabel(\"Purity\")\n",
    "ax3.set_zlabel(\"Harm\")\n",
    "\n",
    "\n",
    "ax4.scatter(df[:, 2], df[:, 4], df[:, 0], c=b, cmap='coolwarm')\n",
    "ax4.set_title('Colour by gender')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395e7043",
   "metadata": {},
   "source": [
    "#### Haidt (day 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd05507",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/12658&studyListingIndex=0_775f45d232bb5e430d0024139e25\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#df = pd.read_csv('GrahamHaidtNosek.2009.JPSP.Study_3.tab', delim_whitespace=True)\n",
    "df = pd.read_csv('GrahamHaidtNosek.2009.JPSP.Study_3.tab',sep=\"\\t\")\n",
    "\n",
    "df = df[['gender', 'HARM_AVG', 'FAIRNESS_AVG', 'INGROUP_AVG', 'AUTHORITY_AVG', 'PURITY_AVG']]\n",
    "df.dropna(axis='rows')\n",
    "\n",
    "arr = df.to_numpy()\n",
    "arr = arr[ ~np.isnan(arr).any(axis=1),:]\n",
    "arr = arr.T\n",
    "\n",
    "for i in range(1, 6):\n",
    "    plt.subplot(3, 2, i) \n",
    "\n",
    "    plt.hist(arr[i], bins=25)\n",
    "    plt.xlabel(list(df.columns.values)[i])\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eda451",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = arr.T\n",
    "\n",
    "for i in range(len(arr.T[0])):\n",
    "    x_bar = sum(arr[i])-arr[i][0]\n",
    "    for j in range(1,6):\n",
    "        arr[i][j] = arr[i][j]/x_bar\n",
    "        \n",
    "df1 = pd.DataFrame(arr)\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.delete(arr.T, 0, 0).T\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=2, random_state=0)\n",
    "model.fit(arr) \n",
    "\n",
    "#y_kmeans = model.predict(arr)\n",
    "\n",
    "a = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e7939",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = np.vstack((arr.T,a))\n",
    "print(df2.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95937cc6",
   "metadata": {},
   "source": [
    "Steps to perform KMeans algorithm by hand, taken from: https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/\n",
    "\n",
    "Step 1: Choose the number of clusters k\n",
    "\n",
    "Step 2: Select k random points from the data as centroids\n",
    "\n",
    "Step 3: Assign all the points to the closest cluster centroid\n",
    "\n",
    "Step 4: Recompute the centroids of newly formed clusters\n",
    "\n",
    "Step 5: Repeat steps 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b5b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "import numpy.ma as ma\n",
    "\n",
    "def m2(arrA, arrB):\n",
    "    return np.sum(np.power((arrA-arrB),2))\n",
    "\n",
    "def label_closest_centroid_arr(arr, centroid_arr):\n",
    "    closest_centroid_arr = np.zeros(len(arr), dtype=bool)\n",
    "    for i in range(len(arr)):\n",
    "            if m2(arr[i], centroid_arr[0]) < m2(arr[i], centroid_arr[1]):\n",
    "                closest_centroid_arr[i] = 1\n",
    "            else: \n",
    "                closest_centroid_arr[i] = 0\n",
    "    return closest_centroid_arr\n",
    "\n",
    "def return_next_centroid(arr, centroid_arr):\n",
    "    closest_centroid_arr = label_closest_centroid_arr(arr, centroid_arr)\n",
    "\n",
    "    x = np.array(arr.T)\n",
    "    y = np.array(closest_centroid_arr)\n",
    "    z = np.array(np.invert(closest_centroid_arr))\n",
    "\n",
    "    centroid_arr[0] = x[:, y].mean(axis=1)\n",
    "    centroid_arr[1] = x[:, z].mean(axis=1)\n",
    "\n",
    "    return centroid_arr\n",
    "\n",
    "def kmeans_iteration():\n",
    "    k = 2\n",
    "    s = sample(range(len(arr)), k)\n",
    "\n",
    "    centroid_arr = [arr[s[0]], arr[s[1]]]\n",
    "    closest_centroid_arr = np.zeros(len(arr), dtype=bool)\n",
    "    for i in range(300):\n",
    "        temp = np.array(return_next_centroid(arr, centroid_arr))\n",
    "        if m2(temp, centroid_arr) < 0.0001:\n",
    "            break\n",
    "        centroid_arr = temp\n",
    "    return centroid_arr\n",
    "\n",
    "# Does NOT generalize to more than k=2 clusters\n",
    "def inertia(arr, centroid_arr):\n",
    "    sm = 0\n",
    "    for i in range(len(arr)):\n",
    "        sm = sm + min(np.linalg.norm(arr[i]-centroid_arr[0].astype(float), ord=2)**2,\n",
    "                      np.linalg.norm(arr[i]-centroid_arr[1].astype(float), ord=2)**2)\n",
    "    return sm\n",
    "\n",
    "\n",
    "for i in range(0, 9):\n",
    "    min_inertia = float(\"inf\")\n",
    "    kmi_temp = kmeans_iteration()\n",
    "    print(inertia(arr, kmi_temp))\n",
    "    if inertia(arr, kmi_temp) < min_inertia:\n",
    "        kmi = kmi_temp\n",
    "        a = label_closest_centroid_arr(arr, kmi).astype(int)\n",
    "        min_inertia = inertia(arr, kmi)\n",
    "        \n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=2, random_state=5)\n",
    "model.fit(arr) \n",
    "\n",
    "y_kmeans = model.predict(arr)\n",
    "\n",
    "b = model.labels_\n",
    "print(kmi)\n",
    "print(m2(a,b))\n",
    "print(model.cluster_centers_)\n",
    "print(inertia(arr, model.cluster_centers_))\n",
    "print(model.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca86050e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = np.vstack((arr.T,a))\n",
    "print(df2.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe44c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"ticks\")\n",
    "sns.pairplot(pd.DataFrame(df2.T), hue=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593fca90",
   "metadata": {},
   "source": [
    "#### Haidt (day 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/12658&studyListingIndex=0_775f45d232bb5e430d0024139e25\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#df = pd.read_csv('GrahamHaidtNosek.2009.JPSP.Study_3.tab', delim_whitespace=True)\n",
    "df = pd.read_csv('GrahamHaidtNosek.2009.JPSP.Study_3.tab',sep=\"\\t\")\n",
    "\n",
    "df = df[['gender', 'HARM_AVG', 'FAIRNESS_AVG', 'INGROUP_AVG', 'AUTHORITY_AVG', 'PURITY_AVG']]\n",
    "df.dropna(axis='rows')\n",
    "\n",
    "arr = df.to_numpy()\n",
    "arr = arr[ ~np.isnan(arr).any(axis=1),:]\n",
    "arr = arr.T\n",
    "\n",
    "'''for i in range(1, 6):\n",
    "    plt.subplot(3, 2, i) \n",
    "\n",
    "    plt.hist(arr[i], bins=25)\n",
    "    plt.xlabel(list(df.columns.values)[i])\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04022d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = arr.T\n",
    "\n",
    "for i in range(len(arr.T[0])):\n",
    "    x_bar = sum(arr[i])-arr[i][0]\n",
    "    for j in range(1,6):\n",
    "        arr[i][j] = arr[i][j]/x_bar\n",
    "        \n",
    "df1 = pd.DataFrame(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f3410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set_theme(style=\"ticks\")\n",
    "#sns.pairplot(df1, hue=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2450672",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.delete(arr.T, 0, 0).T)\n",
    "#print(df1)\n",
    "arr = np.delete(arr.T, 0, 0).T\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=2, random_state=0)\n",
    "model.fit(arr) \n",
    "\n",
    "y_kmeans = model.predict(arr)\n",
    "\n",
    "a = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af8f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = np.delete(arr.T, 0, 0)\n",
    "df2 = np.vstack((arr.T,a))\n",
    "print(df2.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6be775",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"ticks\")\n",
    "sns.pairplot(pd.DataFrame(df2.T), hue=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b1c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://dataverse.harvard.edu/dataset.xhtml?persistentId=hdl:1902.1/12658&studyListingIndex=0_775f45d232bb5e430d0024139e25\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('GrahamHaidtNosek.2009.JPSP.Study_3.tab',sep=\"\\t\")\n",
    "df = df[['pol_liberal', 'pol_moderate', 'pol_conservative', 'HARM_AVG', 'FAIRNESS_AVG', 'INGROUP_AVG', 'AUTHORITY_AVG', 'PURITY_AVG']]\n",
    "\n",
    "for i in range(len(list(df.columns))):\n",
    "    df = df[pd.notnull(df[df.columns[i]])]\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13440d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = df.to_numpy()\n",
    "\n",
    "#print(arr)\n",
    "\n",
    "print(\"Liberals:\\t \" + str(sum((arr.T)[0])))\n",
    "print(\"Moderates:\\t \" + str(sum((arr.T)[1])))\n",
    "print(\"Conservatives:\\t \" + str(sum((arr.T)[2])))\n",
    "\n",
    "print(len(arr))\n",
    "\n",
    "pol_arr = ((arr.T)[0:3]).T\n",
    "mor_arr = ((arr.T)[3:8]).T\n",
    "con_arr = ((arr.T)[2]).T\n",
    "\n",
    "mor_arr = mor_arr / np.mean(mor_arr, axis=1)[:,None] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e50024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "plt.hist(mor_arr.T[0], bins=100, density=True)\n",
    "plt.xlabel(\"Harm\")\n",
    "\n",
    "mu = np.mean(mor_arr.T[0])\n",
    "std = np.std(mor_arr.T[0])\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = norm.pdf(x, mu, std)\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "title = \"HARM_AVG for overall\\n popluation: mu = %.2f,  std = %.2f\" % (mu, std)\n",
    "plt.title(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1526ecf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X = mor_arr[0:3000]\n",
    "y = con_arr[0:3000]\n",
    "\n",
    "model = LogisticRegression(random_state=0).fit(X, y)\n",
    "\n",
    "model.score(X, y)\n",
    "\n",
    "print(\"Model predicted %d conservatives and %d liberals\" %\n",
    "      (sum(model.predict(mor_arr[3000:])), len(mor_arr[3000:])\n",
    "       -sum(model.predict(mor_arr[3000:]))))\n",
    "print(\"There were %d conservatives and %d liberals\" %\n",
    "      (sum(con_arr[3000:]), sum(1 - con_arr[3000:]))) \n",
    "\n",
    "confusion_matrix(model.predict(mor_arr[3000:]), con_arr[3000:])\n",
    "\n",
    "# 2368 Liberal and liberal\n",
    "# 266 guessed liberal but was conservative\n",
    "# 55 vice versa\n",
    "# 321 guessed conservative and was conservative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b42e7e",
   "metadata": {},
   "source": [
    "### NLP - Carbuying Forum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a17f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "\n",
    "filename=open(\"posts2.txt\",\"r\")\n",
    "\n",
    "\n",
    "tokens = []\n",
    "for line in filename.readlines():\n",
    "    tokens +=  [x.lower() for x in nltk.word_tokenize(line.replace(\"'\",\"\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[x.lower() for x in tokens]\n",
    "import re\n",
    "filtered_tokens = [token for token in tokens if re.search('[a-zA-Z|0-9|\\.]', token)]\n",
    "filtered_tokens2 = [t for t in filtered_tokens if len(t) > 1 or t == '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc70444a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "filtered_tokens3 = []\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "for word in filtered_tokens2:\n",
    "    if word.casefold() not in stop_words:\n",
    "         filtered_tokens3.append(word)\n",
    "print(filtered_tokens3[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd16080",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens4 = filtered_tokens3\n",
    "\n",
    "for i in range(len(filtered_tokens3)):\n",
    "    filtered_tokens4[i] = filtered_tokens3[i].rstrip('s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d3a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_tokens4[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec87f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import split_after\n",
    "\n",
    "flt5 = list(split_after(filtered_tokens4[1:], lambda x: x == \".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b26bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "\n",
    "bigram = Phrases(flt5, min_count=1, threshold=3)\n",
    "\n",
    "fng = []\n",
    "for i in range(len(flt5)):\n",
    "    fng.append(bigram[flt5[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6460349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=fng, vector_size=200, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061c7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(model.wv.key_to_index.keys())[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab90b16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(\"living_toronto\", topn=8))\n",
    "print(\"\\n\")\n",
    "print(model.wv.most_similar(\"im_considering\", topn=8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b2bc6",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eceecb",
   "metadata": {},
   "source": [
    "$r=\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sqrt{\\sum\\left(x_{i}-\\bar{x}\\right)^{2} \\sum\\left(y_{i}-\\bar{y}\\right)^{2}}}$\n",
    "\n",
    "$r=\\frac{\\operatorname{cov}(X, Y)}{\\sigma_{X} \\sigma_{Y}}$\n",
    "\n",
    "$y_{i}=\\beta_{0}+\\beta_{1} x_{i}+\\epsilon_{i} \\quad i=1, \\ldots, n$\n",
    "\n",
    "find $\\hat{\\beta}_{0}, \\hat{\\beta}_{1}$ that minimizes the sum of squares of the errors $\\sum_{i=1}^{n} \\epsilon_{i}^{2}$. Ie minimize\n",
    "\n",
    "$S\\left(\\beta_{0}, \\beta_{1}\\right)=\\sum_{i=1}^{n} \\epsilon_{i}^{2}=\\sum_{i=1}^{n}\\left[y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)\\right]^{2}$\n",
    "\n",
    "$\\frac{\\partial S}{\\partial \\beta_{0}}=-2 \\sum_{i=1}^{n} \\left(y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)\\right)=0$\n",
    "\n",
    "$\\frac{\\partial S}{\\partial \\beta_{1}}=-2 \\sum_{i=1}^{n} x_{i}\\left[y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i}\\right)\\right]=0$\n",
    "\n",
    "Rearranging first we get\n",
    "\n",
    "$\\sum_{n=1}^{n} y_{i}=n \\hat{\\beta}_{0}+\\sum_{n=1}^{n} x_{i} \\hat{\\beta}_{1}$\n",
    "\n",
    "$\\hat{\\beta}_{0}=\\bar{y}-\\hat{\\beta}_{1} \\bar{x}$\n",
    "\n",
    "And second we get \n",
    "\n",
    "$\\sum_{n=1}^{n} x_{i} y_{i}=\\sum_{i=1}^{n} x_{i} \\hat{\\beta}_{0}+\\sum_{n=1}^{n} x_{i}^{2} \\hat{\\beta}_{1}$\n",
    "\n",
    "$\\hat{\\beta}_{1} = \\frac{\\sum_{n=1}^{n} x_{i} y_{i}-\\sum_{i=1}^{n} x_{i} \\hat{\\beta}_{0}}{\\sum_{n=1}^{n} x_{i}^{2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a7c49",
   "metadata": {},
   "source": [
    "$\\frac{\\partial S}{\\partial \\beta_{1}}=-2 \\sum_{i=1}^{n} x_{i}\\left[y_{i}-\\left(\\bar{y}-\\hat{\\beta}_{1} \\bar{x}+\\beta_{1} x_{i}\\right)\\right]=0$\n",
    "\n",
    "$\\sum_{i=1}^{n} x_{i} y_{i} = \\sum x_i \\bar{y}+\\hat{\\beta}_{1} \\sum_{i=1}^{n}x_i(x_{i} - \\bar{x})$\n",
    "\n",
    "$\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} x_{i} y_{i}-\\sum x_{i} \\bar{y}}{\\sum_{i=1}^{n} x_{i}\\left(x_{i}-\\bar{x}\\right)}$\n",
    "\n",
    "$\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^{n} (x_{i} - \\bar{x}) y_{i}-\\sum (x_{i} - \\bar{x}) \\bar{y}}{\\sum_{i=1}^{n} (x_{i} - \\bar{x})\\left(x_{i}-\\bar{x}\\right)}=\\frac{\\sum\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}{\\sum\\left(x_{i}-\\bar{x}\\right)^{2}} = \\frac{S_{x y}}{S_{x x}}$\n",
    "\n",
    "Thus we get\n",
    "\n",
    "$\\hat{\\beta}_{0}=\\bar{y}-\\hat{\\beta}_{1} \\bar{x}$\n",
    "\n",
    "$\\hat{\\beta}_{1}=\\frac{S_{x y}}{S_{x x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d358216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_datareader as pdr\n",
    "\n",
    "tickers = ['msft', 'tqqq']\n",
    "\n",
    "df = pdr.DataReader(tickers, data_source='yahoo',\n",
    "                    start='2017-01-01', end='2020-09-28')['Adj Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b88809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.arange(941)\n",
    "y = df[\"msft\"]\n",
    "\n",
    "arr = np.full((2, 941), 0)\n",
    "\n",
    "arr[0] = x\n",
    "arr[1] = y\n",
    "\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0e51f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of squared difference S_xy\n",
    "def S(x, y):\n",
    "    return np.sum((x-np.mean(x))*(y-np.mean(y)))\n",
    "\n",
    "beta_1 = S(x,y)/S(x,x)\n",
    "beta_0 = np.mean(y) - beta_1*np.mean(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f632b544",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(0,940,941)\n",
    "y = beta_1*x+beta_0\n",
    "plt.plot(x, y, '-r')\n",
    "plt.plot(arr[0][0::10], arr[1][0::10])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614d4a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr4 = np.concatenate((arr, [np.array(df[\"tqqq\"])]), axis=0)\n",
    "\n",
    "plt.plot(arr4[0],arr4[1])\n",
    "plt.plot(arr4[0],arr4[2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41072a5",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119796be",
   "metadata": {},
   "source": [
    "n observations and p explanitory covariates:\n",
    "\n",
    "$S\\left(\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{p}\\right)=\\sum\\left[y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i 1}+\\ldots+\\beta_{p} x_{i p}\\right)\\right]^{2}$\n",
    "\n",
    "Setting partials to $0$:\n",
    "\n",
    "$\\begin{aligned}\n",
    "&\\frac{\\partial S}{\\partial \\beta_{0}}=-2 \\sum x_{i 0}\\left[y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i 1}+\\ldots+\\beta_{p} x_{i p}\\right)\\right]=0 \\\\\n",
    "&\\frac{\\partial S}{\\partial \\beta_{1}}=-2 \\sum x_{i 1}\\left[y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i 1}+\\ldots+\\beta_{p} x_{i p}\\right)\\right]=0 \\\\\n",
    "&\\vdots \\\\\n",
    "&\\frac{\\partial S}{\\partial \\beta_{p}}=-2 \\sum x_{i p}\\left[y_{i}-\\left(\\beta_{0}+\\beta_{1} x_{i 1}+\\ldots+\\beta_{p} x_{i p}\\right)\\right]=0\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8a459",
   "metadata": {},
   "source": [
    "$\\begin{aligned}\n",
    "&\\left(\\sum_{i=1}^n x_{i 0}^2\\right)\\beta_{0}+\\left(\\sum_{i=1}^n x_{i 0} x_{i 1}\\right) \\beta_{1}+\\ldots+\\left(\\sum_{i=1}^n x_{i 0} x_{i p}\\right) \\beta_{p}=\\sum_{i=1}^n x_{i 0} y_{i} \\\\\n",
    "&\\left(\\sum_{i=1}^n x_{i 0} x_{i 1}\\right) \\beta_{0}+\\left(\\sum_{i=1}^n x_{i 1}^{2}\\right) \\beta_{1}+\\ldots+\\left(\\sum_{i=1}^n x_{i 1} x_{i p}\\right) \\beta_{p}=\\sum_{i=1}^n x_{i 1} y_{i} \\\\\n",
    "&\\vdots \\\\\n",
    "&\\left(\\sum_{i=1}^n x_{i 0} x_{i p}\\right) \\beta_{0}+\\left(\\sum_{i=1}^n x_{i 1} x_{i p}\\right) \\beta_{1}+\\ldots+\\left(\\sum_{i=1}^n x_{i p}^{2}\\right) \\beta_{p}=\\sum_{i=1}^n x_{i p} y_{i}\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea3663",
   "metadata": {},
   "source": [
    "$X^T X \\hat{\\beta} = X^Ty$\n",
    "\n",
    "$\\hat{\\beta} = \\left(X^{T} X\\right)^{-1} X^{T} y$, assuming $X$ is non-singular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e695706",
   "metadata": {},
   "source": [
    "$y$ is $n$ x $1$\n",
    "\n",
    "$X$ is $n$ x $(p+1)$,  \n",
    "\n",
    "$\\beta$ is $(p+1)$ x $1$\n",
    "\n",
    "$X^T X$ is $p+1$ by $p+1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4909984a",
   "metadata": {},
   "source": [
    "### simple Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85cf803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = tf.keras.utils.normalize(x_train, axis=1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis=1)\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(16, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5283fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = model.evaluate(x_test, y_test)[0]\n",
    "val_acc = model.evaluate(x_test, y_test)[1]\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52586e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([x_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e8ad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(np.argmax(predictions[3]))\n",
    "\n",
    "plt.imshow(x_train[3], cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834dd55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w = model.get_layer('flatten').get_weights()\n",
    "#np.array(w[0]).shape\n",
    "#print(w[0][0])\n",
    "\n",
    "#m = np.array_split(w[0].T[11], 28)\n",
    "\n",
    "#plt.imshow(m, cmap=plt.cm.binary)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82bebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"d4 = model.get_layer('dense_3').get_weights()\n",
    "\n",
    "print(d4[0][0])\n",
    "\n",
    "for i in range(16):\n",
    "    md4 = np.array_split(d4[0].T[i], 28)\n",
    "    plt.imshow(md4)\n",
    "    plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bb60ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "topEdge = np.zeros(784, dtype=float)\n",
    "\n",
    "for i in range(len(topEdge)):\n",
    "    if i % 28 in range(10, 19) and i // 28 in range(6, 8):\n",
    "        topEdge[i] = 1\n",
    "    if i % 28 in range(10, 19) and i // 28 in [5, 8]:\n",
    "        topEdge[i] = -.3\n",
    "\n",
    "\n",
    "plt.imshow(np.array_split(topEdge, 28), cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9fbf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "leftEdge = np.zeros(784, dtype=float)\n",
    "\n",
    "for i in range(len(leftEdge)):\n",
    "    if i // 28 in range(10, 19) and i % 28 in range(6, 8):\n",
    "        leftEdge[i] = 1\n",
    "    if i // 28 in range(10, 19) and i % 28 in [5, 8]:\n",
    "        leftEdge[i] = -.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afa5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "rightEdge = np.zeros(784, dtype=float)\n",
    "\n",
    "for i in range(len(rightEdge)):\n",
    "    if i // 28 in range(10, 19) and i % 28 in range(17, 19):\n",
    "        rightEdge[i] = 1\n",
    "    if i // 28 in range(10, 19) and i % 28 in [16, 19]:\n",
    "        rightEdge[i] = -.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b32dbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))\n",
    "\n",
    "sigmoid(x_train[0].flatten()@topEdge.flatten())\n",
    "\n",
    "tempArr = []\n",
    "for i in range(len(x_train)):\n",
    "    tempArr.append(sigmoid(x_train[i].flatten()@topEdge.flatten()))\n",
    "\n",
    "tempArr = np.array(tempArr)\n",
    "    \n",
    "plt.hist(tempArr, bins=100)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dd6d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([0, 0, 0, 0, 1, 0, 0, 0])\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "#softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee28e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pdt(k):\n",
    "    l1 = x_train[k].flatten()\n",
    "    \n",
    "    l2 = np.array([[sigmoid(l1 @ topEdge)],\n",
    "             [sigmoid(l1 @ leftEdge)],\n",
    "                  [sigmoid(l1 @ rightEdge)]])\n",
    "    \n",
    "    l3 = np.array([[0, 0, 0], #0\n",
    "          [0, 0, 0],\n",
    "          [0, 0, 0],\n",
    "          [0, 0, 0], #3\n",
    "          [0, 1.22, 0],\n",
    "          [1.05, 0, 0],\n",
    "          [0, 0, 0], #6\n",
    "          [.5, 0, .5],\n",
    "          [.4,.3, .4], #8\n",
    "          [0,0,0]])\n",
    "\n",
    "    return np.argmax(l2.T @ l3.T)\n",
    "\n",
    "m = 0\n",
    "for i in range(len(x_train)):\n",
    "    if y_train[i] == pdt(i):\n",
    "        m += 1\n",
    "        \n",
    "print(m / len(x_train))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tmpArr = []\n",
    "for i in range(1000, 1050):\n",
    "    #print(pdt(i))\n",
    "    #plt.imshow(x_train[i], cmap=plt.cm.binary)\n",
    "    #plt.show()\n",
    "    tmpArr.append(pdt(i))\n",
    "\n",
    "plt.hist(tmpArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e4a017",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d15a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90f19dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(iris['data'][:5,:])\n",
    "#print(iris['target'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db8ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "X = np.array(iris['data'])\n",
    "y = np.array(iris['target'])\n",
    "X, y = shuffle(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cc8cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820835e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "    \n",
    "def K_nearest(p, K):\n",
    "    dist_from_p = np.linalg.norm(p*np.ones_like(X_train) - X_train, axis=1)\n",
    "    nearest_to_p_indices = np.argsort(dist_from_p)[:K]\n",
    "    mode = stats.mode(y_train[nearest_to_p_indices])[0][0]\n",
    "    return mode\n",
    "\n",
    "def annotate_KNN(K, max_iter=float('inf')):\n",
    "    for i in range(len(X_test)):\n",
    "        if i >= max_iter:\n",
    "            return        \n",
    "        prd = K_nearest(X_test[i], K)\n",
    "        lab = y_test[i]\n",
    "        if prd == lab:\n",
    "            print(\"\\x1B[30mK nearest prediction: \" + str(prd) +\n",
    "                  \"\\t Actual label \" + str(lab))\n",
    "        else:\n",
    "            print(\"\\x1b[31mK nearest prediction: \" + str(prd) +\n",
    "                  \"\\t Actual label \" + str(lab))\n",
    "    return\n",
    "\n",
    "def accuracy_KNN(K):\n",
    "    acc = np.zeros_like(y_test)\n",
    "    for i in range(len(X_test)):\n",
    "        prd = K_nearest(X_test[i], K)\n",
    "        lab = y_test[i]\n",
    "        if prd == lab:\n",
    "            acc[i] = 1\n",
    "    return np.sum(acc)/len(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf18cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate_KNN(2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398bca94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_arr = np.ones(100)\n",
    "for i in range(1, len(acc_arr)):\n",
    "    acc_arr[i] = accuracy_KNN(i)\n",
    "\n",
    "plt.plot(acc_arr)\n",
    "plt.show()\n",
    "\n",
    "acc_arr = np.ones(10)\n",
    "for i in range(1, len(acc_arr)):\n",
    "    acc_arr[i] = accuracy_KNN(i)\n",
    "\n",
    "plt.plot(acc_arr)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "gist_id": "c18446785bd06c741919f53399685000",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "789px",
    "left": "37px",
    "top": "111.125px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
